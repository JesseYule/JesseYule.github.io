<!DOCTYPE html>
<!-- saved from url=(0029)https://carrac.co.jp/service/ -->
<html lang="ja"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Jesse Yule's Blog</title>
    <meta name="viewport" content="width=device-width">

    <style>.l-page { visibility: hidden; }</style>
    <link rel="stylesheet" as="style" href="../../css/main/main.min.css">
    <link rel="apple-touch-icon" sizes="180x180" href="https://carrac.co.jp/img/common/app_icon.png">

    <link rel="stylesheet" href="../../css/menu/common.css" media="all">
    <link rel="stylesheet" href="../../css/menu/home.css" media="all">
    <script type="text/javascript" async="" src="../../css/menu/analytics.js"></script>
    <script async="" src="../../css/menu/js"></script>


    <meta name="theme-color" content="#ffffff">
    <!-------------------------matjax------------------------------>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>
    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
    <!---------------------------------------------------------------->
    <script type="text/javascript" async="" src="../../css/main/analytics.js"></script><script async="" src="../../css/main/gtm.js"></script><script>
      (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
      new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
      j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
      'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
      })(window,document,'script','dataLayer','GTM-KHDRBVP');
    </script>
    <!-- End Google Tag Manager -->
  </head>
  <body style="font-family: 等线, 等线 Light,'微软雅黑 Light',Helvetica Neue, Helvetica, Arial, PingFang SC; font-weight: lighter ;word-break: break-all ;background-color: #323638 ">
    <!-- Google Tag Manager (noscript) -->
    <noscript>
      &lt;iframe src="https://www.googletagmanager.com/ns.html?id=GTM-KHDRBVP"
      height="0" width="0" style="display:none;visibility:hidden"&gt;
      &lt;/iframe&gt;
    </noscript>
    <!-- End Google Tag Manager (noscript) -->
    
    <div class="l-page" data-page-id="service">
      <!-- Start to define the global header.-->
      <div class="l-gh"></div>
      <!-- End to define the global header.-->




      <div class="l-contents">
        <!-- Start to define the main content.-->

          <header class="st-hdr" id="js-hdr">
              <div class="hdr-bar m-row m-row--jc--spacebetween m-row--ai--center">

              </div>

          </header>

          <button class="trigger" id="js-trigger"><span></span><span></span><span></span></button>



          <nav class="overlay-navigation" style="color: #eeeeee">
              <div class="overlay-navigation__inner m-row">
                  <div class="overlay-navigation__block">
                      <ul class="sitemap m-row m-row--fw--wrap">

                              <li class="sitemap__unit"><a href="../../index.html">Home</a></li>

                          <li class="sitemap__unit"><a href="../../about/about.html">About</a></li>

                          <li class="sitemap__unit"><a href="../../programming/java1.html">Java</a></li>



                          <li class="sitemap__unit"><a href="../../math/all1.html">Mathematics</a></li>

                          <li class="sitemap__unit"><a href="../../naturallanguage/all1.html">Natural Language</a></li>

                          <li class="sitemap__unit"><a href="../../machinelearning/ml1.html">Machine Learning</a></li>

<li class="sitemap__unit"><a href="../../articles/all1.html">Others</a></li>

                      </ul>

                  </div>
                  <div class="sns"><h2 class="overlay-navigation__siteID"><img src="../../css/menu/img_siteID-m.svg"
                                                                               alt=""></h2></div>
              </div>
          </nav>



          <div class="p-lower">
          <div class="c-red-line c-slide-in u-animdel-100"></div>
          <h2 class="p-lower__title-en c-slide-in u-animdel-100">transformer公式推导</h2>

		  
		   <div class="p-lower-kv">
            <div class="p-lower-kv__text">
              <div class="p-lower-kv__subtitle">  </div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2">本文主要按照transformer对数据处理的流程，分析各个步骤transformer模型的处理细节和公式推导。</div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2"><br></div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2">我们有一个句子，首先要做的就是embedding，一般来说可以使用预训练的词向量（比如Glove），把句子转换成向量。</div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2"><br></div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2">因为transformer模型不像CNN、RNN那样可以分析序列的位置（顺序）信息，所以一开始我们要针对序列进行positional encoding：</div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2"><section style="text-align: center; margin:0 auto">$$PE_p{(pos, 2i)} = sin(pos/10000^{2i/d_{model}})$$</section></div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2"><section style="text-align: center; margin:0 auto">$$PE_p{(pos, 2i+1)} = cos(pos/10000^{2i/d_{model}})$$</section></div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2">从公式可以看出，位置编码会得到一段和词向量同样长度的位置编码向量，向量第2i个元素为正弦函数，第2i+1个元素为余弦函数，比如pos=3（句子的第三个词），d_model=128(词向量长度）：</div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2"><section style="text-align: center; margin:0 auto">$$[sin(3/10000^{0/128}), cos(3/10000^{1/128}), sin(3/10000^{2/128})...] $$</section></div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2">不同pos的词向量会得到不同频率的正弦余弦函数生成的位置编码向量，因为和词向量长度相同，就可以直接叠加到词向量上。</div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2"><br></div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2">完成了positional encoding，序列就传送到transformer的encoder层，encoder层包括多个encoder（一般6个），一层encoder的输出作为另一层encoder的输入，最后的输出传送到decoder。</div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2"><br></div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2">encoder包括一个multi-head attention和position-wise feed forward network。multi-head attention就是多个self attention：</div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2"><section style="text-align: center; margin:0 auto">$$Attention（Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}}V)$$</section></div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2">所谓self attention，就是对输入自身进行注意力计算，首先，句子中的各个词向量并行输入到multi-head，然后每个词向量会进行线性变换，生成三个新的向量QKV，之后再进行注意力计算。所谓multi-head，就是对每个词向量，会生成多组QKV，进行多次的注意力计算，最后再将结果合并在一起，并通过权重矩阵（WO）把维度调整到和原输入一样：</div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2"><section style="text-align: center; margin:0 auto">$$MultiHead(Q,K,V) = Concat(head_1, ..., head_h)W^O$$</section></div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2"><section style="text-align: center; margin:0 auto">$$head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)$$</section></div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2">之所以要把最后的输入通过权重矩阵调整为和原输入一样，是因为后面的feed forward是逐个位置计算的，也就是说如果heads不同，输出维度也不同，就要每次调整feed forward的超参数了，所以这里选择每次把multihead的输出调整为和原输入一样。</div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2"><br></div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2">multihead的输出不是直接输入到feed forward，而是首先经过残差连接和层级归一化，这两个操作在feed forward的输出也有用到，主要避免梯度消失等问题：</div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2"><section style="text-align: center; margin:0 auto">$$LayerNorm(x+Sublayer(x))$$</section></div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2">Sublayer就是指multihead或者feed forward这些运算。</div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2"><br></div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2">接下来就轮到feed forward，一般会使用两个线性变换和一个ReLU激活函数</div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2"><section style="text-align: center; margin:0 auto">$$FFN(x) = max(0, xW_1 + b_1)W_2+b_2$$</section></div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2">我认为feed forward的主要作用是降维和特征提取。另外要注意的是，feed forward在同一层是参数共享的，也就是不同位置的词向量都是输入到同一个feed forward网络，但是不同层就不共享参数，顺带一提，之前的multihead中不同的head很明显也是参数不共享的。</div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2"><br></div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2">对于decoder来说，一个不同点是decoder会接受历史时刻的输出作为输入，这时候就要求模型在训练时不能不小心读取了未来时刻的序列数据，我们通过sequence mask来实现。主要是产生一个上三角矩阵，上三角全为1，下三角全为0，作用到序列中，就可以达到我们的目的了。</div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2"><br></div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2">基本上这些就是transformer中涉及到的细节，模型具体怎么运作、不同层不同函数的功能之前的文章也有详细叙述，这里不再复述。</div>

            </div>

        <!-- End to define the main content.-->
      </div>


    </div>
    <script src="../../css/main/main.min.js" async=""></script>

    <script type="text/javascript" defer=""
            src="../../css/menu/autoptimize_73b1aaeb49de3f7372d04614ffa9ecd3.js"></script>

</body></html>