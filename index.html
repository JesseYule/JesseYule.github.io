<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <title>Jesse Yule's Blog</title>
    <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
            new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
        j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
        'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-PLTDDQK');</script>
    <!-- End Google Tag Manager -->
    <meta name="viewport" content="width=device-width,initial-scale=1.0,maximum-scale=1.0,user-scalable=0">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <script>new ViewportExtra(375)</script>
     <link rel="stylesheet" href="css/newmain/lib.css">
    <link rel="stylesheet" href="css/newmain/app.css">



</head>

<body class="page-template-default page page-id-194">



<div class="partnership-page">



    <div class="single">
        <div class="single__multiple-header">

            <h1 class="single__multiple-header__title"><br></h1>

            <h1 class="single__multiple-header__title">Junjie Yu（Jesse Yule）</h1>
        </div>

        <div class="single__content single__content--margin-multiple js-single__content">




            <p>本科毕业于华南农业大学数学与应用数学专业，研究生毕业于悉尼麦考瑞大学精算专业，目前主要研究深度学习与自然语言处理。</p>
            <ul>
                <li>主邮箱：Jesse.Junjie.Yu@foxmail.com</li>
                <li>副邮箱：jesseyulejunjie@gmail.com</li>
                <li>github：JesseYule</li>
                <li>csdn: JessssseYule</li>
            </ul>
            <p><br></p>



            <h2>研究项目</h2>

            <h4>基于Transformer的增强型序列推断模型（ESIM）</h4>

            <p><strong>项目简介</strong>
            </p>
            <ul>

                <ul>
                    <li>Transformer作为特征提取器，与RNN（LSTM）、CNN相比，不存在长期依赖问题，可高效并行计算，且不须构建过深的网络也能取得较好的效果</li>
                    <li>主要思路：1. 以判断重复文本为例，两个输入序列依次通过预训练的glove词向量、Transformer的Encoder结构进行两次Embedding；2. 利用注意力机制计算两个句子词与词之间的相似度，以此构建权重，进一步对句子间相似度进行局部推断得到信息向量；3. 利用Transformer的Encoder结构对信息向量进行特征提取和降维；4.进行平均池化和最大池化，确保不同长度的序列转化为同样尺寸的向量； 5. 利用全连接层做分类，判断是否重复</li>
                    <li>模型不足： 在其他应用场景，如问答系统，注意力机制会带来大量运算量，目前亦有提出一种动态注意力机制提高运算效率</li>
                    <li><a href="https://github.com/JesseYule/DuplicatedQuestionDetection" target="_blank" rel="noopener noreferrer">github项目地址</a></li>

                </ul>

                <!--<ul class="list">-->
                    <!--<li><a href="https://github.com/JesseYule/DuplicatedQuestionDetection" target="_blank" rel="noopener noreferrer">github项目地址</a></li>-->
                <!--</ul>-->

                <!--<ul class="list">-->
                    <!--<li><a href="projects/DuplicatedQuestionDetection/content.html" target="_blank" rel="noopener noreferrer">项目详细介绍</a></li>-->
                <!--</ul>-->





                <h4>基于RNN卷积核的Network in Network模型</h4>

                <p><strong>项目简介</strong>
                </p>

                <ul>
                    <li>和LeNet（传统CNN）相比，Network in Network模型用微型网络（MLP）作为卷积核，增强了卷积核提取非线性特征的能力（原始卷积核只能提取线性特征）</li>
                    <li>在自然语言处理领域，考虑到位置信息对文本数据的重要性，首次提出使用RNN作为卷积核，与MLP相比可保留位置信息之余，卷积核尺寸较小解决了RNN的长期依赖问题，卷积运算的特性使得RNN可进行并行计算</li>
                    <li>主要思路：1. 通过遮掩掉序列的部分词向量，针对不同时刻生成不同子序列（每段子序列可看作包含各个时刻的词向量，不同子序列包含的范围不同）；2. 对上一子序列的输出和当前子序列进行线性变换及求和，进行卷积运算得到当前子序列的输出；3. 线性变换采用大小为1的卷积核进行卷积运算，可大大减少参数量</li>
                    <li><a href="https://github.com/JesseYule/NetworkInNetwork-RNNKernel-" target="_blank" rel="noopener noreferrer">github项目地址</a></li>

                </ul>

                <!--<ul class="list">-->
                    <!--<li><a href="https://github.com/JesseYule/NetworkInNetwork-RNNKernel-" target="_blank" rel="noopener noreferrer">github项目地址</a></li>-->
                <!--</ul>-->

                <!--<ul class="list">-->
                    <!--<li><a href="projects/NetworkInNetwork-RNNKernel-/content.html" target="_blank" rel="noopener noreferrer">项目详细介绍</a></li>-->
                <!--</ul>-->




                <h2>个人博客</h2>

                <p>主要内容包括统计推断、数据科学、深度学习、自然语言处理等等，主要记录了在学习过程中的思考和总结</p>

                <!--<aside class="link-card link-card&#45;&#45;margin">-->

                    <!--<ul class="link-card__lists">-->
                        <!--<li class="link-card__lists__item">-->
                            <!--<a href="https://www.vext.co.jp/about/" class="link-card__lists__item__inner">-->
                                <!--<p class="link-card__lists__item__title">about Vext</p>-->
                                <!--<p class="link-card__lists__item__text">ベクストについて</p>-->
                            <!--</a>-->
                        <!--</li>-->
                        <!--<li class="link-card__lists__item">-->
                            <!--<a href="https://www.vext.co.jp/case/" class="link-card__lists__item__inner">-->
                                <!--<p class="link-card__lists__item__title">Case Studies</p>-->
                                <!--<p class="link-card__lists__item__text">導入事例</p>-->
                            <!--</a>-->
                        <!--</li>-->
                        <!--<li class="link-card__lists__item">-->
                            <!--<a href="https://www.vext.co.jp/seminar/" class="link-card__lists__item__inner">-->
                                <!--<p class="link-card__lists__item__title">Seminer / Event</p>-->
                                <!--<p class="link-card__lists__item__text">セミナー・イベントのご案内</p>-->
                            <!--</a>-->
                        <!--</li>-->
                    <!--</ul>-->

                <!--</aside>-->


                <p><a class="button button--large-sp button--center"
                      href="index2.html" target="_blank"
                      rel="noopener noreferrer"><span class="button__text">浏览博客</span></a><br>

                </p>

        </div>


    </div>













</div><!-- /.wrap -->

</body>
</html>
