<!DOCTYPE html>
<!-- saved from url=(0029)https://carrac.co.jp/service/ -->
<html lang="ja"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Jesse Yule's Blog</title>
    <meta name="viewport" content="width=device-width">

    <style>.l-page { visibility: hidden; }</style>
    <link rel="stylesheet" as="style" href="../../css/main/main.min.css">
    <link rel="apple-touch-icon" sizes="180x180" href="https://carrac.co.jp/img/common/app_icon.png">

    <link rel="stylesheet" href="../../css/menu/common.css" media="all">
    <link rel="stylesheet" href="../../css/menu/home.css" media="all">
    <script type="text/javascript" async="" src="../../css/menu/analytics.js"></script>
    <script async="" src="../../css/menu/js"></script>


    <meta name="theme-color" content="#ffffff">
    <!-------------------------matjax------------------------------>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>
    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
    <!---------------------------------------------------------------->
    <script type="text/javascript" async="" src="../../css/main/analytics.js"></script><script async="" src="../../css/main/gtm.js"></script><script>
      (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
      new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
      j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
      'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
      })(window,document,'script','dataLayer','GTM-KHDRBVP');
    </script>
    <!-- End Google Tag Manager -->
  </head>
  <body style="font-family: 等线, 等线 Light,'微软雅黑 Light',Helvetica Neue, Helvetica, Arial, PingFang SC; font-weight: lighter ;word-break: break-all ;background-color: #323638 ">
    <!-- Google Tag Manager (noscript) -->
    <noscript>
      &lt;iframe src="https://www.googletagmanager.com/ns.html?id=GTM-KHDRBVP"
      height="0" width="0" style="display:none;visibility:hidden"&gt;
      &lt;/iframe&gt;
    </noscript>
    <!-- End Google Tag Manager (noscript) -->
    
    <div class="l-page" data-page-id="service">
      <!-- Start to define the global header.-->
      <div class="l-gh"></div>
      <!-- End to define the global header.-->




      <div class="l-contents">
        <!-- Start to define the main content.-->

          <header class="st-hdr" id="js-hdr">
              <div class="hdr-bar m-row m-row--jc--spacebetween m-row--ai--center">

              </div>

          </header>

          <button class="trigger" id="js-trigger"><span></span><span></span><span></span></button>



          <nav class="overlay-navigation" style="color: #eeeeee">
              <div class="overlay-navigation__inner m-row">
                  <div class="overlay-navigation__block">
                      <ul class="sitemap m-row m-row--fw--wrap">

                              <li class="sitemap__unit"><a href="../../index.html">Home</a></li>

                          <li class="sitemap__unit"><a href="../../about/about.html">About</a></li>

                          <li class="sitemap__unit"><a href="../../programming/java1.html">Java</a></li>



                          <li class="sitemap__unit"><a href="../../math/all1.html">Mathematics</a></li>

                          <li class="sitemap__unit"><a href="../../naturallanguage/all1.html">Natural Language</a></li>

                          <li class="sitemap__unit"><a href="../../machinelearning/ml1.html">Machine Learning</a></li>

<li class="sitemap__unit"><a href="../../articles/all1.html">Others</a></li>

                      </ul>

                  </div>
                  <div class="sns"><h2 class="overlay-navigation__siteID"><img src="../../css/menu/img_siteID-m.svg"
                                                                               alt=""></h2></div>
              </div>
          </nav>



          <div class="p-lower">
          <div class="c-red-line c-slide-in u-animdel-100"></div>
          <h2 class="p-lower__title-en c-slide-in u-animdel-100">Neural Turing Machines</h2>

		  
		   <div class="p-lower-kv">
            <div class="p-lower-kv__text">
              <div class="p-lower-kv__subtitle">  </div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2">之前提到的孪生网络那类模型，简单来说他们都是把训练集作为先验知识进行建模，而像LSTM、神经图灵机这一类模型则提供了另一个新思路，那就是引入外部记忆，在训练过程中通过训练集对外部记忆进行修改，从而在测试的时候利用外部记忆作为先验知识。</div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2"><br></div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2"></div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2"><br></div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2"><br></div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2"><br></div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2"><br></div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2"><br></div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2"><br></div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2"><br></div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2"><br></div>

                <div class="p-lower-kv__outline p-lower-kv__outline--col2">在介绍什么是神经图灵机之前，先来看看什么是图灵机：用机器模拟人们用纸笔进行数学运算的过程，图灵把这个过程看成两个简单的动作，一个是在纸上写上或擦除某个符号，一个是把注意力从一个位置移到另一个位置。</div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2"><br></div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2">看着觉得普普通通的思想，可是回想一下我们进行任意计算的过程，却又是符合这种思想，图灵把所有的数学运算抽象成这两个动作，不得不说真的厉害。</div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2"><br></div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2">而神经图灵机，就是通过深度学习的模型，根据图灵机的思想构建的模型。首先，我们可以把问题看成，对模型输入一串数字，然后模型根据这串数字输出一个计算结果，数字的长度每次都是不定的，我们马上就想到了RNN和LSTM。</div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2"><br></div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2">我们可以近似把神经图灵机看成LSTM的改进版本，但需要知道的是，LSTM能够实现图灵机的什么功能，又有什么不足需要改进，这可以帮助我们更好地理解神经图灵机。</div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2"><br></div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2">首先，LSTM可以依次读取任意长度的输入得到输出，模拟我们的计算过程，但是有一种情况是LSTM做不到的，比如</div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2"><br></div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2"><br></div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2"><br></div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2">其实RNN已经符合图灵机的定义了，接下来就是怎么改进RNN而已，说到底就是怎么改善梯度消失的问题，LSTM通过引入长期记忆进行改进，而神经图灵机则更进一步，把长期记忆称为内存池，然后每次根据输入和内存池进行计算。所以本质上，神经图灵机就是一个改进版的LSTM。</div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2"><br></div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2">还记得LSTM根据输入，对长期记忆进行遗忘和写入，最后再根据输入和更新了的长期记忆计算输出，个人觉得其实神经图灵机也是十分相似的过程，只是针对长期记忆以及遗忘、写入这两个操作进行改进（好像也没有对长期记忆进行实质上的改进，只是换了个名称）：</div>
                <div class="p-service-index-map__image c-fade-in-up js-scroll-item is-shown">
                    <picture>
                        <source media="(max-width: 568px)" srcset="image/1.png"><img src="image/1.png" alt="ポジショニングマップ">
                    </picture>
                </div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2">其实这个过程真的和LSTM是一摸一样的，而且我觉得LSTM的图还没这个清晰：</div>
                <div class="p-service-index-map__image c-fade-in-up js-scroll-item is-shown">
                    <picture>
                        <source media="(max-width: 568px)" srcset="image/2.jpeg"><img src="image/2.jpeg" alt="ポジショニングマップ">
                    </picture>
                </div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2">神经图灵机的内存池就相当于LSTM的长期记忆，也就是上面那条线。LSTM是根据当前输入先遗忘再写入最后读取最新的长期记忆，而神经图灵机其实也是有这三个部分，只是把遗忘和写入整合为写操作。</div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2"><br></div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2">先来看看神经图灵机的读操作，在LSTM中，模型是根据当前的输入，读取更新后的长期记忆中有用的信息，而神经图灵机也是差不多的思路，首先在时间t，内存池，或者说记忆矩阵Mt是一个N*M的矩阵，N是每个记忆的位置，M是记忆向量（或者说具体的记忆信息），wt就是时间t的关于这N个记忆的权重向量，那么在时间t，读取的记忆就是：</div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2"><section style="text-align: center; margin:0 auto">$$r_t = \sum_i w_t(i) M_t(i)$$</section></div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2">其实这也是一种注意力机制，根据注意力（权重）从记忆中读取相关的内容。</div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2"><br></div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2">现在的问题就是权重怎么求，一般有两种方法，第一种就是最常见的通过输入和记忆的相似度计算权重，这种方法以前讨论过很多次这里就不再详述，主要来看一下为什么我们还需要另一种方法，在论文中提到一个例子，计算两个数的乘积f(x,y)=x*y，在第一个时刻，我们输入了x，然后写入到内存池中，在第二个时刻，我们要十分准确地从内存池中读取x的值，这时候我们如果还是采用相似度的计算方法，就有可能造成读取不准确，所以论文就提出可以采用location-based addressing的方法（前者称为content-based addressing）。</div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2"><br></div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2">关于这个location-based addressing还是要详细讲一下。在实际应用中，location-based addressing其实求的是一个位移权重，假设目前的权重为wt，考虑wt中每个位置元素wt(i)的相邻k个元素，比如3个，那么我们就有wt(i-1)、wt(i)、wt(i+1)，然后我们会确定这三者的权重s，通过线性组合计算出当前位置的新权重：</div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2"><section style="text-align: center; margin:0 auto">$$w'_t(i) = \sum_{j=-1}^1 w_t(i+j)s(j)$$</section></div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2">按照我的理解，这个location-based addressing必须要先有一个权重矩阵（初始化矩阵），再根据位移权重对原来的权重矩阵进行修改，不然我们也没有办法求出位移权重。另一方面，这个权重位移公式作为相邻三个位置的权重的线性组合，在最极端的情况可以直接替换相邻位置的权重，可是也有一个问题就是，能够替换的位置只限制在相邻k个。</div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2"><br></div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2">实际上，神经图灵机结合两种方法计算权重：</div>
                <div class="p-service-index-map__image c-fade-in-up js-scroll-item is-shown">
                    <picture>
                        <source media="(max-width: 568px)" srcset="image/3.png"><img src="image/3.png" alt="ポジショニングマップ">
                    </picture>
                </div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2">首先针对当前输入和内存池，计算相似度，得出权重矩阵。然后，对当前的权重矩阵和上一时刻的权重矩阵进行插值（线性组合）：</div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2"><section style="text-align: center; margin:0 auto">$$w_t^g = g_t w_t^c + (1-g_t) w_{t-1}$$</section></div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2">主要目的就是看看当前需不需要Content-based Addressing，比如之前提到的特殊情况我们只能利用location-based addressing计算权重，这时候我们就不能采用Content-based Addressing计算的权重，只需要设置gt=0就能达到这个效果。最后我们就可以计算位移权重。</div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2"><br></div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2">值得注意的是，计算完位移权重之后，得到的仍然不是最终的输出，因为考虑到一种情况，如果位移权重计算的结果比较平均，就会导致数据的分散和泄漏，或者这样理解，考虑一种最极端的情况，每个位移权重都等于相邻三个位置的均值，就会导致最终的权重差异过小，所以最后我们还需要增强权值之间的差异程度，也就是sharping，主要通过生成一个大于1的参数，对权重进行指数运算后再归一化：</div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2"><section style="text-align: center; margin:0 auto">$$w_t(i) = \frac{w'_t(i)^{\gamma _t}}{\sum_j w'_t(i)^ {\gamma _t} }$$</section></div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2">以上就是计算权重的全过程。</div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2"><br></div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2">上面讨论了怎么读取记忆和怎么计算权重，最后就剩下怎么消除记忆和写入记忆，其实也是和LSTM非常相似，首先controller输出一个取值范围在01之间的消除向量et，通过消除向量遗忘记忆：</div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2"><section style="text-align: center; margin:0 auto">$$M'_t(i) = M_{t-1}(i)(1-w_t(i)e_t(i))$$</section></div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2">另一方看，controller也会输出一个取值范围在01之间的增加向量at，在执行完记忆消除后增加新的记忆：</div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2"><section style="text-align: center; margin:0 auto">$$M_t(i) = M'_t(i) + w_t(i)a_t(i)$$</section></div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2">其中的权重也是通过上述提到的方法求得。</div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2"><br></div>
                <div class="p-lower-kv__outline p-lower-kv__outline--col2">最后再完整说一次整个神经图灵机的运作流程，首先初始化内存池矩阵、读写头向量等等，controller接受当前输入和初始化得到的上一时刻的输入（读取内存池），计算输出，然后利用输出更改内存池（包含记忆的消除与写入），重复以上步骤，得到最终的输出。</div>


            </div>

        <!-- End to define the main content.-->
      </div>


    </div>
    <script src="../../css/main/main.min.js" async=""></script>

    <script type="text/javascript" defer=""
            src="../../css/menu/autoptimize_73b1aaeb49de3f7372d04614ffa9ecd3.js"></script>

</body></html>